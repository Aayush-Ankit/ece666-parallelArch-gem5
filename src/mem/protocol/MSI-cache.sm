/* Create the state machine file for L1 cache controller */

// define an object for L1Cache controller
machine(MachineType:L1Cache, "MSI cache") //name the sm file: L1Cache and add a description
    // PARAMETERS (params. to state machine, passed directly to SimObject generated by sm file)
    : Sequencer *sequencer; // iterfaces Ruby to rest of gem5
      CacheMemory *cacheMemory;
      bool send_evictions; // used in OOO cores to notify LSQ if address is evicted after a load (to squash load if it was speculative)
      Cycles cache_response_latency := 12;
      Cycles issue_latency := 2;

      // MESSAGE BUFFERS (interface between state machine and ruby network)
      // Need three virtual networks to avoid deadlock (virtual_network 2,3,4: higher number has higher priority)
      MessageBuffer * requestFromCache, network="To", virtual_network="2",
            vnet_type="request";
      MessageBuffer * responseFromCache, network="To", virtual_network="4",
            vnet_type="response";

      MessageBuffer * forwardToCache, network="From", virtual_network="3",
            vnet_type="forward";
      MessageBuffer * responseToCache, network="From", virtual_network="4",
            vnet_type="response";

      MessageBuffer * mandatoryQueue; // used by sequencer to convert gem5 packets to ruby requres (not connected to ruby network)

{
    // STATES (stable and trasient, acesspermissions are used for functional access to cache/debug-like access)
    state_declaration(State, desc="Cache states") {
        I, AccessPermission:Invalid, desc="Not Present/Invalid";

        // States moving out of I
        IS_D, AccessPermission:Invalid, desc="Invalid, moving to S. Waiting for data";
        IM_AD, AccessPermission:Invalid, desc="Invalid, moving to M. Waiting for acks and data";
        IM_A, AccessPermission:Busy, desc="Invalid, moving to M. Waiting for acks";

        S, AccessPermission:Read_Only, desc="Shared. Other caches may have the block";

        // States moving out of S
        SM_AD, AccessPermission:Read_Only, desc="Shared, moving to M. Waiting for acks and data";
        SM_A, AccessPermission:Read_Only, desc="Shared, moving to M. Waiting for acks";

        M, AccessPermission:Read_Write, desc="Modified. Owner of block";

        // States moving to I
        // Challenge: directory can receive a GETM from another cache before requestor cache's PUTM
        // need to differentiate: hence directory sends either ack or nack (as no point-to-point ordering assumed)
        MI_A, AccessPermission:Busy, desc="Modified, moving to I. Waiting for put-ack or put-nack";
        MII_A, AccessPermission:Busy, desc="Modified, moving to I, received a later put-nack. Waiting for earlier fwd-getm/fed-gets";
        SI_A, AccessPermission:Busy, desc="Shared, moving to I. Waiitng for put ack";
        II_A, AccessPermission:Invalid, desc="Sent valid data, sees Inv or GetM. Waiitng for put-nack";
    }

    // EVENTS
    enumeration(Event, desc="Cache events") {
        // From processor/sequencer/mandatory queue
        Load, desc="Load from processor";
        Ifetch, desc="Ifetch from processor";
        Store, desc="Store from processor";

        // Internal event (trigger from another processor request)
        Replacement, desc="Triggered when block is chosen as victim";

        // Forwarded request from other cache via dir on the forward network
        FwdGetS, desc="Directory sent us a request to satisfy GetS. We must have the block in M to respond to this.";
        FwdGetM, desc="Directory sent us a request to satisfy GetM. We must have the block in M to respond to this.";
        Inv, desc="Invalidate from the directory.";
        // Challenge: Why PutAck/PutNack in forward network ?
        PutAck, desc="Response from directory after we issue a put.";
        PutNack, desc="Response from directory after we issue a put, intercepted by GetM/GetS at dir.";

        // Responses from directory
        DataDirNoAcks,  desc="Data from directory (acks = 0)";
        DataDirAcks,    desc="Data from directory (acks > 0)";

        // Responses from other caches
        DataOwner,      desc="Data from owner";
        InvAck,         desc="Invalidation ack from other cache after Inv";

        // Special event to simplify implementation
        LastInvAck,     desc="Triggered after the last ack is received";
    }

    // STRUCTURE DEFINITIONS
    // CacheEntry - structure stored in CacheMemory
    structure(Entry, desc="...", interface="AbstractCacheEntry") {
      State CacheState,        desc="cache state";
      DataBlock DataBlk,       desc="Data in the block";
    }

    // TBE fields - structure to store trasient states/data (like MSHR)
    structure(TBE, desc="...") {
      State TBEState, desc="Transient state";
      DataBlock DataBlk, desc="data for the block, required for concurrent writebacks: in MI_A)";
      int AcksOutstanding, default=0, desc="Number of acks left to receive";
    }

    structure(TBETable, external="yes") {
      TBE lookup(Addr);
      void allocate(Addr);
      void deallocate(Addr);
      bool isPresent(Addr);
    }

    // STRUCTURES
    TBETable TBEs, template="<L1Cache_TBE>", constructor="m_number_of_TBEs";

    // PROTOTYPES
    Tick clockEdge();
    Cycles ticksToCycles(Tick t);
    void set_cache_entry(AbstractCacheEntry a);
    void unset_cache_entry();
    void set_tbe(TBE b);
    void unset_tbe();
    void profileMsgDelay(int virtualNetworkType, Cycles b);
    MachineID mapAddressToMachine(Addr addr, MachineType mtype);

    Entry getCacheEntry(Addr address), return_by_pointer="yes" {
      return static_cast(Entry, "pointer", cacheMemory.lookup(address));
    }

    // FUNCTIONS
    Event mandatory_request_type_to_event(RubyRequestType type) {
     if (type == RubyRequestType:LD) {
        return Event:Load;
      } else if (type == RubyRequestType:IFETCH) {
        return Event:Ifetch;
      } else if ((type == RubyRequestType:ST) || (type == RubyRequestType:ATOMIC)) {
        return Event:Store;
      } else {
        error("Invalid RubyRequestType");
      }
    }

    State getState(TBE tbe, Entry cache_entry, Addr addr) {
      // TBE state ovverrides the state in cache memory, if valid
      if (is_valid(tbe)) {
        return tbe.TBEState;
      }
      else if (is_valid(cache_entry)) {
        return cache_entry.CacheState;
      }
      else {
        return State:I;
      }
    }

    void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {
      if (is_valid(tbe)) {
        tbe.TBEState := state;
      }
      if (is_valid(cache_entry)) {
        cache_entry.CacheState := state;
      }
    }

    AccessPermission getAccessPermission(Addr addr) {
      TBE tbe := TBEs[addr];
      if(is_valid(tbe)) {
        return L1Cache_State_to_permission(tbe.TBEState);
      }

      Entry cache_entry := getCacheEntry(addr);
      if(is_valid(cache_entry)) {
        return L1Cache_State_to_permission(cache_entry.CacheState);
      }

      return AccessPermission:NotPresent;
    }

    void setAccessPermission(Entry cache_entry, Addr addr, State state) {
      if (is_valid(cache_entry)) {
        cache_entry.changePermission(L1Cache_State_to_permission(state));
      }
    }

    void functionalRead(Addr addr, Packet *pkt) {
      TBE tbe := TBEs[addr];
      if(is_valid(tbe)) {
        testAndRead(addr, tbe.DataBlk, pkt);
      } else {
        testAndRead(addr, getCacheEntry(addr).DataBlk, pkt);
      }
    }

    int functionalWrite(Addr addr, Packet *pkt) {
      int num_functional_writes := 0;

      TBE tbe := TBEs[addr];
      if(is_valid(tbe)) {
        num_functional_writes := num_functional_writes +
          testAndWrite(addr, tbe.DataBlk, pkt);
        return num_functional_writes;
      }

      num_functional_writes := num_functional_writes +
          testAndWrite(addr, getCacheEntry(addr).DataBlk, pkt);
      return num_functional_writes;
    }

    // NETWORK PORTS (in ports specify what events to trigger on diff. incoming messages)
    // in_ports code_blocks are executed in order. NOTE: earlier virtual_network specification is ignored by ruby,
    // except that messages on network 2 can only go to message buffers on network 2 (can't jump networks)
    out_port(requestNetwork_out, RequestMsg, requestFromCache);
    out_port(responseNetwork_out, ResponseMsg, responseFromCache);

    // response in_port has highest priority
    // NOTE: Within in_port code block, we should check all logic conditions
    // NOTE: upon triggering an event only one code path (no if statement in action block), if so use more states or events
    // NOTE: transitions in ruby are atomic, either all actions or none
    in_port(responseNetwork_in, ResponseMsg, responseToCache) {
      if (responseNetwork_in.isReady(clockEdge())) {
        // peek is a special function: grabs the msg and puts in special variable-in_msg
        // Challenge: what does block_on here give? (block_on ensures that anyother request to same cacheline is blocked until currect request completes)
        peek(responseNetwork_in, ResponseMsg, block_on="addr") {
          Entry cache_entry := getCacheEntry(in_msg.addr);
          TBE tbe := TBEs[in_msg.addr];
          // for TBE, it's response to earlier request generated by cache controller, there must be valid TBE in TBE table
          assert(is_valid(tbe)); // for DEBUG

          // If it's from the directory
          if (machineIDToMachineType(in_msg.Sender) == MachineType:Directory) {
              if (in_msg.Type != CoherenceResponseType:Data) {
                  error("Directory should only reply with data"); // for DEBUG
              }
              assert (in_msg.Acks + tbe.AcksOutstanding >= 0); //number of acks we are waiting can't be <0
              if (in_msg.Acks + tbe.AcksOutstanding == 0) {
                  trigger(Event:DataDirNoAcks, in_msg.addr, cache_entry, tbe);
              }
              else {
                  trigger(Event:DataDirAcks, in_msg.addr, cache_entry, tbe);
              }
          }
          // This is from other cache
          else {
              if (in_msg.Type == CoherenceResponseType:Data) {
                trigger(Event:DataOwner, in_msg.addr, cache_entry, tbe);
              }
              else if (in_msg.Type == CoherenceResponseType:InvAck) {
                DPRINTF(RubySlicc, "Got inv ack. %d left\n", tbe.AcksOutstanding);
                if (tbe.AcksOutstanding == 1) {
                    // If there is exactly one ack remaining then we
                    // know it is the last ack.
                    trigger(Event:LastInvAck, in_msg.addr, cache_entry, tbe);
                }
                else {
                    trigger(Event:InvAck, in_msg.addr, cache_entry, tbe);
                }
              }
              else {
                error("Unexpected response from other cache");
              }
          }

        }
      }
    }

    // forward request in_port (one event for each request type, only comes from directory)
    in_port(forwardRequestNetwork_in, RequestMsg, forwardToCache) {
        if (forwardRequestNetwork_in.isReady(clockEdge())) {
            // Challenge: what does block_on here give?
            peek(forwardRequestNetwork_in, RequestMsg, block_on="addr") {
                // Grab the entry and tbe if they exist
                Entry cache_entry := getCacheEntry(in_msg.addr);
                TBE tbe := TBEs[in_msg.addr];

                if (in_msg.Type == CoherenceRequestType:GetS) {
                    trigger(Event:FwdGetS, in_msg.addr, cache_entry, tbe);
                } else if (in_msg.Type == CoherenceRequestType:GetM) {
                    trigger(Event:FwdGetM, in_msg.addr, cache_entry, tbe);
                } else if (in_msg.Type == CoherenceRequestType:Inv) {
                    trigger(Event:Inv, in_msg.addr, cache_entry, tbe);
                } else if (in_msg.Type == CoherenceRequestType:PutAck) {
                    trigger(Event:PutAck, in_msg.addr, cache_entry, tbe);
                } else if (in_msg.Type == CoherenceRequestType:PutNack) {
                    trigger(Event:PutNack, in_msg.addr, cache_entry, tbe);
                } else {
                    error("Unexpected forward message!");
                }

            }
        }
    }

    // Mandatory Queue
    in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...") {
      if (mandatoryQueue_in.isReady(clockEdge())) {
        peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {
          Entry cache_entry := getCacheEntry(in_msg.LineAddress);
          TBE tbe := TBEs[in_msg.LineAddress];

          if (is_invalid(cache_entry) && cacheMemory.cacheAvail(in_msg.LineAddress) == false ) {
            // make room for the block - block not present, no cacheline available
            Addr addr := cacheMemory.cacheProbe(in_msg.LineAddress);
            Entry victim_entry := getCacheEntry(addr);
            TBE victim_tbe := TBEs[addr];
            trigger(Event:Replacement, addr, victim_entry, victim_tbe);
          }
          else {
            trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, tbe);
          }
        }
      }
    }

    // ACTIONS
    // address, cache_entry, tbe_entry are implicitly passed to action from transition

    // actions for get and put requests from processor
    action(sendGetS, 'gS', desc="Send GetS to the directory") {
        enqueue(requestNetwork_out, RequestMsg, issue_latency) {
            //out_msg is a special variable, just like in_msg in peek function
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:GetS;
            out_msg.Destination.add(mapAddressToMachine(address, MachineType:Directory));
            // See mem/protocol/RubySlicc_Exports.sm for possible sizes.
            out_msg.MessageSize := MessageSizeType:Control;
            // Set that the requestor is this machine so we get the response.
            out_msg.Requestor := machineID;
        }
    }

    action(sendGetM, "gM", desc="Send GetM to the directory") {
        enqueue(requestNetwork_out, RequestMsg, issue_latency) {
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:GetM;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Control;
            out_msg.Requestor := machineID;
        }
    }

    action(sendPutS, "pS", desc="Send PutS to the directory") {
        enqueue(requestNetwork_out, RequestMsg, issue_latency) {
            assert(is_valid(cache_entry));
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:PutS;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Control;
            out_msg.Requestor := machineID;
        }
    }

    action(sendPutM, "pM", desc="Send putM+data to the directory") {
        enqueue(requestNetwork_out, RequestMsg, issue_latency) {
            assert(is_valid(cache_entry));
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:PutM;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.DataBlk := cache_entry.DataBlk;
            out_msg.MessageSize := MessageSizeType:Data;
            out_msg.Requestor := machineID;
        }
    }

    // actions to respond to forward requests
    action(sendCacheDataToReq, "cdR", desc="Send cache data to requestor") {
        assert(is_valid(cache_entry));
        peek(forwardRequestNetwork_in, RequestMsg) {
            enqueue(responseNetwork_out, ResponseMsg, cache_response_latency) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:Data;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Data;
                out_msg.Sender := machineID;
            }
        }
    }

    action(sendCacheDataToDir, "cdD", desc="Send the cache data to the dir") {
        enqueue(responseNetwork_out, ResponseMsg, cache_response_latency) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:Data;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.DataBlk := cache_entry.DataBlk;
            out_msg.MessageSize := MessageSizeType:Data;
            out_msg.Sender := machineID;
        }
    }

    action(sendInvAcktoReq, "iaR", desc="Send inv-ack to requestor") {
        peek(forwardRequestNetwork_in, RequestMsg) {
            enqueue(responseNetwork_out, ResponseMsg, 1) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:InvAck;
                out_msg.Destination.add(in_msg.Requestor);
                //out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Control;
                out_msg.Sender := machineID;
            }
        }
    }

    // actions to manage acknowledgements
    action(decrAcks, "da", desc="Decrement the number of acks") {
        assert(is_valid(tbe));
        tbe.AcksOutstanding := tbe.AcksOutstanding - 1;
        APPEND_TRANSITION_COMMENT("Acks: ");
        APPEND_TRANSITION_COMMENT(tbe.AcksOutstanding);
    }

    action(storeAcks, "sa", desc="Store the needed acks to the TBE") {
        assert(is_valid(tbe));
        peek(responseNetwork_in, ResponseMsg) {
            tbe.AcksOutstanding := in_msg.Acks + tbe.AcksOutstanding;
        }
        assert(tbe.AcksOutstanding > 0);
    }

    // actions to respon dto CPU on hit/miss
    action(loadHit, "Lh", desc="Load hit") {
        assert(is_valid(cache_entry));
        DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
        cacheMemory.setMRU(cache_entry);
        sequencer.readCallback(address, cache_entry.DataBlk, false);
    }

    action(externalLoadHit, "xLh", desc="External load hit (was a miss)") {
        assert(is_valid(cache_entry));
        peek(responseNetwork_in, ResponseMsg) {
            DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
            cacheMemory.setMRU(cache_entry);
            // Forward the type of machine that responded to this request
            // E.g., another cache or the directory. This is used for tracking
            // statistics (C2C or mem).
            sequencer.readCallback(address, cache_entry.DataBlk, true,
                                   machineIDToMachineType(in_msg.Sender));
        }
    }

    action(storeHit, "Sh", desc="Store hit") {
        assert(is_valid(cache_entry));
        DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
        cacheMemory.setMRU(cache_entry);
        // The same as the read callback above.
        sequencer.writeCallback(address, cache_entry.DataBlk, false);
    }

    action(externalStoreHit, "xSh", desc="External store hit (was a miss)") {
        assert(is_valid(cache_entry));
        peek(responseNetwork_in, ResponseMsg) {
            DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
            cacheMemory.setMRU(cache_entry);
            sequencer.writeCallback(address, cache_entry.DataBlk, true,
                                   // Note: this could be the last ack.
                                   machineIDToMachineType(in_msg.Sender));
        }
    }

    action(forwardEviction, "e", desc="sends eviction notification to CPU") {
        if (send_evictions) {
            sequencer.evictionCallback(address);
        }
    }

    // actions to allocate/deallocate cache/tbe
    action(allocateCacheBlock, "a", desc="Allocate a cache block") {
        assert(is_invalid(cache_entry));
        assert(cacheMemory.cacheAvail(address));
        set_cache_entry(cacheMemory.allocate(address, new Entry));
    }

    action(deallocateCacheBlock, "d", desc="Deallocate a cache block") {
        assert(is_valid(cache_entry));
        cacheMemory.deallocate(address);
        // clear the cache_entry variable (now it's invalid)
        unset_cache_entry();
    }

    action(writeDataToCache, "wd", desc="Write data to the cache") {
        peek(responseNetwork_in, ResponseMsg) {
            assert(is_valid(cache_entry));
            cache_entry.DataBlk := in_msg.DataBlk;
        }
    }

    action(allocateTBE, "aT", desc="Allocate TBE") {
        assert(is_invalid(tbe));
        TBEs.allocate(address);
        // this updates the tbe variable for other actions
        set_tbe(TBEs[address]);
    }

    action(deallocateTBE, "dT", desc="Deallocate TBE") {
        assert(is_valid(tbe));
        TBEs.deallocate(address);
        // this makes the tbe variable invalid
        unset_tbe();
    }

    action(copyDataFromCacheToTBE, "Dct", desc="Copy data from cache to TBE") {
        assert(is_valid(cache_entry));
        assert(is_valid(tbe));
        tbe.DataBlk := cache_entry.DataBlk;
    }

    // Actions to manage the message buffers
    action(popMandatoryQueue, "pQ", desc="Pop the mandatory queue") {
        mandatoryQueue_in.dequeue(clockEdge());
    }

    action(popResponseQueue, "pR", desc="Pop the response queue") {
        Tick delay := responseNetwork_in.dequeue(clockEdge());
        profileMsgDelay(1, ticksToCycles(delay));
    }

    action(popForwardQueue, "pF", desc="Pop the forward queue") {
        Tick delay := forwardRequestNetwork_in.dequeue(clockEdge());
        profileMsgDelay(2, ticksToCycles(delay));
    }

    // action to stall (a protocol stall in in_port logic stalls all messages from being
    // processed in the current message buffer and all lower priority message buffer
    action(stall, "z", desc="Stall the incoming request") {
        // z_stall
    }

    // actions to profile cache misses/hits
    action(p_profileMiss, "pi", desc="Profile cache miss") {
        ++cacheMemory.demand_misses;
    }

    action(p_profileHit, "ph", desc="Profile cache miss") {
        ++cacheMemory.demand_hits;
    }


    // TRANSITIONS (transition happens in atomic fashion)
    transition(I, {Load, Ifetch}, IS_D) {
        allocateCacheBlock;
        allocateTBE;
        sendGetS;
        popMandatoryQueue;
    }

    transition(I, Store, IM_AD) {
        allocateCacheBlock;
        allocateTBE;
        sendGetM;
        popMandatoryQueue;
    }

    // No FwdGetS, FwdGetM as dir. knows this requestor isn't owner
    // Inv as dir can send Inv after sending Data (but got reordered in network)
    transition(IS_D, {Load, Ifetch, Store, Replacement, Inv}) {
        stall;
    }

    transition(IS_D, {DataDirNoAcks, DataOwner}, S) {
        writeDataToCache;
        deallocateTBE;
        externalLoadHit;
        popResponseQueue;
    }

    // cannot receive Inv (dir knows either this cache is invalid or M)
    transition({IM_AD, IM_A}, {Load, Ifetch, Store, Replacement, FwdGetS, FwdGetM}) {
        stall;
    }

    transition({IM_AD, SM_AD}, {DataDirNoAcks, DataOwner}, M) {
        writeDataToCache;
        deallocateTBE;
        externalStoreHit;
        popResponseQueue;
    }

    transition(IM_AD, DataDirAcks, IM_A) {
        writeDataToCache;
        storeAcks;
        popResponseQueue;
    }

    transition({IM_AD, IM_A, SM_AD, SM_A}, InvAck) {
        decrAcks;
        popResponseQueue;
    }

    transition({IM_A, SM_A}, LastInvAck, M) {
        deallocateTBE;
        externalStoreHit;
        popResponseQueue;
    }

    transition({S, SM_AD, SM_A, M}, {Load, Ifetch}) {
        loadHit;
        popMandatoryQueue;
    }

    transition(S, Store, SM_AD) {
        allocateTBE;
        sendGetM;
        popMandatoryQueue;
    }

    transition(S, Replacement, SI_A) {
        sendPutS;
        forwardEviction;
    }

    transition(S, Inv, I) {
        sendInvAcktoReq;
        deallocateCacheBlock;
        forwardEviction;
        popForwardQueue;
    }

    transition({SM_AD, SM_A}, {Store, Replacement, FwdGetS, FwdGetM}) {
        stall;
    }

    transition(SM_AD, Inv, IM_AD) {
        sendInvAcktoReq;
        forwardEviction;
        popForwardQueue;
    }

    transition(SM_AD, DataDirAcks, SM_A) {
        writeDataToCache;
        storeAcks;
        popResponseQueue;
    }

    transition(M, Store) {
        storeHit;
        popMandatoryQueue;
    }

    transition(M, Replacement, MI_A) {
        sendPutM;
        forwardEviction;
    }

    transition(M, FwdGetS, S) {
        sendCacheDataToReq;
        sendCacheDataToDir;
        popForwardQueue;
    }

    transition(M, FwdGetM, I) {
        sendCacheDataToReq;
        deallocateCacheBlock;
        popForwardQueue;
    }

    transition({MI_A, SI_A, II_A, MII_A}, {Load, Ifetch, Store, Replacement}) {
        stall;
    }

    transition(MI_A, FwdGetS, SI_A) {
        sendCacheDataToReq;
        sendCacheDataToDir;
        popForwardQueue;
    }

    transition(MI_A, FwdGetM, II_A) {
        sendCacheDataToReq;
        popForwardQueue;
    }

    transition({MI_A, SI_A, II_A}, PutAck, I) {
        deallocateCacheBlock;
        popForwardQueue;
    }

    // add transition for PutNack on MI_A to wait for incoming FwdGetS/FwdGetM
    transition(MI_A, PutNack, MII_A) {
        popForwardQueue;
    }

    // add transition for PutNack at SI_A and II_A states
    transition({SI_A, II_A}, PutNack, I) {
        deallocateCacheBlock;
        popForwardQueue;
    }

    // add transitions for FwdGetM and FwdGetS
    transition(MII_A, FwdGetS, I) {
        sendCacheDataToReq;
        sendCacheDataToDir;
        deallocateCacheBlock;
        popForwardQueue;
    }

    transition(MII_A, FwdGetM, I) {
        sendCacheDataToReq;
        deallocateCacheBlock;
        popForwardQueue;
    }

    transition(SI_A, Inv, II_A) {
        sendInvAcktoReq;
        popForwardQueue;
    }

    // cache can also receive Inv from Dir in I (Cache1 sends PutS, which is seen after Cache0's GetM by Dir.
    // cache1 gets PutAck before the Inv sent by Dir.
    transition(I, Inv) {
        sendInvAcktoReq;
        popForwardQueue;
    }


}



